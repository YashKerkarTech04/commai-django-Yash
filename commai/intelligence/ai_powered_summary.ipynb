{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('samsum-train.csv')\n",
    "test_data = pd.read_csv('samsum-test.csv')\n",
    "val_data = pd.read_csv('samsum-validation.csv')\n",
    "\n",
    "# Reduce dataset size\n",
    "train_sample = train_data.sample(frac=0.2, random_state=42)  # Keep 30% of training data\n",
    "test_sample = test_data.sample(frac=0.4, random_state=42)    # Keep 50% of test data\n",
    "val_sample = val_data.sample(frac=0.4, random_state=42)      # Keep 50% of validation data\n",
    "\n",
    "# Print new dataset sizes\n",
    "# print(len(train_sample), len(test_sample), len(val_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(example):\n",
    "    # Convert None values to empty strings\n",
    "    dialogue = example[\"dialogue\"] if example[\"dialogue\"] is not None else \"\"\n",
    "    summary = example[\"summary\"] if example[\"summary\"] is not None else \"\"\n",
    "\n",
    "    # Tokenize inputs and targets\n",
    "    inputs = tokenizer(\"summarize: \" + dialogue, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    targets = tokenizer(summary, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2946/2946 [00:02<00:00, 1241.21 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 327/327 [00:00<00:00, 1282.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_sample).map(preprocess_data)\n",
    "val_dataset = Dataset.from_pandas(val_sample).map(preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\commai\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # Ensure 'model' is defined\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Ensure 'train_dataset' is defined\n",
    "    eval_dataset=val_dataset  # Ensure 'eval_dataset' is defined\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1107' max='1107' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1107/1107 1:17:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.436146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.832600</td>\n",
       "      <td>0.416606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.462400</td>\n",
       "      <td>0.411782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1107, training_loss=0.6293051873873028, metrics={'train_runtime': 4655.8472, 'train_samples_per_second': 1.898, 'train_steps_per_second': 0.238, 'total_flos': 1196150841409536.0, 'train_loss': 0.6293051873873028, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./commai_summarizer_latest\\\\tokenizer_config.json',\n",
       " './commai_summarizer_latest\\\\special_tokens_map.json',\n",
       " './commai_summarizer_latest\\\\spiece.model',\n",
       " './commai_summarizer_latest\\\\added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./commai_summarizer_latest\")\n",
    "tokenizer.save_pretrained(\"./commai_summarizer_latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_path = \"./commai_summarizer_latest\"\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commai_summary(text):\n",
    "    input_text = \"summarize: \" + text\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=1024).input_ids\n",
    "    output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=150,  # Allows for more detail  \n",
    "    min_length=80,  # Ensures it's longer  \n",
    "    num_beams=5,  \n",
    "    length_penalty=1.0,  # Keeps it balanced (not too short)  \n",
    "    temperature=0.8,  # Adds slight variation  \n",
    "    repetition_penalty=2.0,  # Avoids repeating phrases  \n",
    "    no_repeat_ngram_size=4,  # Prevents redundancy  \n",
    "    early_stopping=True  \n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was thinking about our last discussion. I get what you mean. It happens sometimes. I just felt unsure about how to form my argument properly. But it happens sometimes. We are going to have a discussion on this topic and we will discuss it in the next few weeks. The answer is yes, but there's a lot of confusion. Let's talk about your argument.\n"
     ]
    }
   ],
   "source": [
    "conversation = \"\"\"üìùHey! How are you doing? I was thinking about our last discussion...\n",
    "Yeah, I get what you mean. It happens sometimes! I just felt a bit unsure about how to phrase my argument properly...\"\"\"\n",
    "\n",
    "\n",
    "summary = commai_summary(conversation)\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
